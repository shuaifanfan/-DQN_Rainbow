{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改进的地方时网络架构部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\86185\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\lib.fanfan\\study\\强化学习\\Experiment\\rainbow_4\\dueling_net.ipynb 单元格 2\u001b[0m line \u001b[0;36m<cell line: 218>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=229'>230</a>\u001b[0m epsilon_decay \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2000\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=231'>232</a>\u001b[0m agent \u001b[39m=\u001b[39m DQN(env\u001b[39m=\u001b[39menv,memory_size\u001b[39m=\u001b[39mmemory_size,batch_size\u001b[39m=\u001b[39mbatch_size,target_update\u001b[39m=\u001b[39mtarget_update,epsilon_decay\u001b[39m=\u001b[39mepsilon_decay,seed\u001b[39m=\u001b[39mseed)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=232'>233</a>\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(num_frames)\n",
      "\u001b[1;32me:\\lib.fanfan\\study\\强化学习\\Experiment\\rainbow_4\\dueling_net.ipynb 单元格 2\u001b[0m line \u001b[0;36mDQN.train\u001b[1;34m(self, num_frames, plotting_interval)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m \u001b[39mfor\u001b[39;00m frame_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,num_frames\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=170'>171</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselect_action(state)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m     next_state,reward,done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m     state \u001b[39m=\u001b[39m next_state\n",
      "\u001b[1;32me:\\lib.fanfan\\study\\强化学习\\Experiment\\rainbow_4\\dueling_net.ipynb 单元格 2\u001b[0m line \u001b[0;36mDQN.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     selected_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     selected_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdqn(torch\u001b[39m.\u001b[39;49mFloatTensor(state)\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice))\u001b[39m.\u001b[39margmax() \n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m     \u001b[39m#argmax()默认方法keepdim=False，因此相当于argmax(self.dqn(torch.FloatTensor(state)),dim=0,keepdim=False),返回值是零维张量\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     \u001b[39m#若 argmax(dim=0,keepdim=True)返回值是一维张量\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m     selected_action \u001b[39m=\u001b[39m selected_action\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()    \u001b[39m#将tensor转化为numpy,并且去掉梯度信息\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\86185\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32me:\\lib.fanfan\\study\\强化学习\\Experiment\\rainbow_4\\dueling_net.ipynb 单元格 2\u001b[0m line \u001b[0;36mNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m advantage \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvantage_layer(feature)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_layer(feature)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m q_value \u001b[39m=\u001b[39m value \u001b[39m+\u001b[39m advantage \u001b[39m-\u001b[39m advantage\u001b[39m.\u001b[39;49mmean(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,keepdim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/dueling_net.ipynb#W0sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mreturn\u001b[39;00m q_value\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from typing import Dict,List,Tuple\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,obs_dim:int,size:int,batch_size:int =32):\n",
    "        self.obs_buf = np.zeros([size,obs_dim],dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size,obs_dim],dtype = np.float32)\n",
    "        self.acts_buf = np.zeros([size],dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size],dtype = np.float32)\n",
    "        self.done_buf = np.zeros(size,dtype = np.float32)\n",
    "        self.maxsize,self.batch_size = size,batch_size\n",
    "        self.ptr,self.size = 0,0\n",
    "    def store(\n",
    "            self,\n",
    "            obs:np.ndarray,\n",
    "            act:np.ndarray,\n",
    "            rew:float,\n",
    "            next_obs:np.ndarray,\n",
    "            done:bool\n",
    "    ):\n",
    "        self.obs_buf[self.ptr]=obs\n",
    "        self.next_obs_buf[self.ptr]=next_obs\n",
    "        self.acts_buf[self.ptr]=act\n",
    "        self.rews_buf[self.ptr]=rew\n",
    "        self.done_buf[self.ptr]=done\n",
    "        self.ptr = (self.ptr+1)%self.maxsize\n",
    "        self.size = min(self.size+1,self.maxsize)\n",
    "    def sample_batch(self) -> Dict[str,np.ndarray]:\n",
    "        idxs = np.random.choice(self.size,self.batch_size,replace=False)\n",
    "        return dict(\n",
    "            obs = self.obs_buf[idxs],\n",
    "            next_obs = self.next_obs_buf[idxs],\n",
    "            acts = self.acts_buf[idxs],\n",
    "            rews = self.rews_buf[idxs],\n",
    "            done = self.done_buf[idxs]\n",
    "            )\n",
    "    def __len__(self) ->int:\n",
    "        return self.size\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        super(Network,self).__init__()\n",
    "\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(in_dim,128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.advantage_layer = nn.Sequential(\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,out_dim)\n",
    "        )\n",
    "        self.value_layer = nn.Sequential(\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x:torch.Tensor) ->torch.Tensor:\n",
    "        feature = self.feature_layer(x)\n",
    "        advantage = self.advantage_layer(feature)\n",
    "        value = self.value_layer(feature)\n",
    "        q_value = value + advantage - advantage.mean(dim=-1,keepdim=True)\n",
    "        return q_value\n",
    "\n",
    "class DQN:\n",
    "    def __init__(\n",
    "               self,\n",
    "               env:gym.Env,\n",
    "               memory_size:int,\n",
    "               batch_size:int,\n",
    "               target_update:int,\n",
    "               epsilon_decay:float,\n",
    "               seed:int,\n",
    "               max_epsilon:float =1.0,\n",
    "               min_epsilon:float =0.1,\n",
    "               gamma:float = 0.99\n",
    "     ):\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space.n\n",
    "        self.env = env\n",
    "        self.memory = ReplayBuffer(obs_dim,memory_size,batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.seed = seed\n",
    "        self.epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.gamma = gamma\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.dqn = Network(obs_dim,act_dim).to(self.device)\n",
    "        self.dqn_target = Network(obs_dim,act_dim).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()                                      # target network is not trainable\n",
    "\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(),lr = 1e-3)  #学习率需要调整\n",
    "        self.transition = list()\n",
    "        self.is_test = False\n",
    "    \n",
    "    def select_action(self,state:np.ndarray) ->np.ndarray:\n",
    "         if self.epsilon > np.random.random():\n",
    "             selected_action = self.env.action_space.sample()\n",
    "         else:\n",
    "             selected_action = self.dqn(torch.FloatTensor(state).to(self.device)).argmax() \n",
    "             #argmax()默认方法keepdim=False，因此相当于argmax(self.dqn(torch.FloatTensor(state)),dim=0,keepdim=False),返回值是零维张量\n",
    "             #若 argmax(dim=0,keepdim=True)返回值是一维张量\n",
    "             selected_action = selected_action.detach().cpu().numpy()    #将tensor转化为numpy,并且去掉梯度信息\n",
    "         if not self.is_test:\n",
    "             self.transition = [state,selected_action]          #将state和action存储起来,等待下一步的reward和next_state\n",
    "         return selected_action\n",
    "     \n",
    "    def step(self,action:np.ndarray) -> Tuple[np.ndarray,np.float64,bool]:\n",
    "        next_state,reward,terminated,truncated,_ = self.env.step(action)\n",
    "        done = terminated or truncated                             #done表示是否结束,\n",
    "        if not self.is_test:\n",
    "            self.transition += [reward,next_state,done]            #将reward和next_state,done存储起来\n",
    "            self.memory.store(*self.transition)                    #将transition存储到memory中\n",
    "        return next_state,reward,done\n",
    "    def update_model(self) -> torch.Tensor:\n",
    "        samples = self.memory.sample_batch()\n",
    "        loss = self._compute_dqn_loss(samples)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()  #.item()表示返回tensor中的元素值,不是tensor本身，不是tensor类型，所以不能反向传播\n",
    "    def _compute_dqn_loss(self,samples:Dict[str,np.ndarray]) -> torch.Tensor:\n",
    "        device = self.device\n",
    "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
    "        #下面设个tensor维度重构很重要，因为action,reward,done都是一维的，\n",
    "        #求curr_q_value时，需要按行取值，使用gather()函数，需要保证action的维度是[batch_size,1]\n",
    "        #action从行向量转化为列向量，reshape(-1,1)\n",
    "        action = torch.LongTensor(samples[\"acts\"]).to(device).reshape(-1,1)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"]).to(device).reshape(-1,1)\n",
    "        done = torch.FloatTensor(samples[\"done\"]).to(device).reshape(-1,1)\n",
    "\n",
    "        #计算Q(s_t,a_t),\n",
    "        #网络的输出层维度是action数目\n",
    "        #每次计算loss，都是一个批次的数据计算，在这里假设是cartpole，action数目为2，所以输出层维度为2\n",
    "        #self.dqn(state).shape = [batch_size,2],action.shape = [batch_size,1],按照gather，表示取每一行的action对应的值\n",
    "        #因此，curr_q_value.shape = [batch_size,1]\n",
    "        curr_q_value = self.dqn(state).gather(1,action)  #gather(1,action)表示按行取action对应的值\n",
    "        #计算Q(s_t+1,a_t+1)\n",
    "        next_q_value = self.dqn_target(next_state).max(dim=1,keepdim=True)[0].detach() #dim=1表示按行取最大值,[0]表示取值,[1]表示取索引\n",
    "        \n",
    "        mask = 1-done\n",
    "        target = (reward+self.gamma*next_q_value*mask).to(self.device)\n",
    "        loss = F.smooth_l1_loss(curr_q_value,target)\n",
    "        return loss\n",
    "    def train(self,num_frames,plotting_interval=200):\n",
    "        self.is_test = False\n",
    "        state,_ = self.env.reset(seed=self.seed)\n",
    "        update_cnt = 0\n",
    "        epsilons = []\n",
    "        losses = []\n",
    "        scores = []\n",
    "        score = 0\n",
    "\n",
    "        for frame_idx in range(1,num_frames+1):\n",
    "            action = self.select_action(state)\n",
    "            next_state,reward,done = self.step(action)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                state,_ = self.env.reset(seed=self.seed)\n",
    "                scores.append(score)\n",
    "                score = 0\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                loss = self.update_model()\n",
    "                losses.append(loss)\n",
    "                update_cnt += 1\n",
    "\n",
    "                self.epsilon = max(self.min_epsilon,self.epsilon - (self.max_epsilon-self.min_epsilon)*self.epsilon_decay)\n",
    "                epsilons.append(self.epsilon)\n",
    "\n",
    "                if update_cnt % self.target_update == 0:\n",
    "                    self._target_hard_update()\n",
    "            \n",
    "            if frame_idx % plotting_interval == 0:\n",
    "                self._plot(frame_idx,scores,losses,epsilons)\n",
    "        \n",
    "        self.env.close()\n",
    "    \n",
    "    def _target_hard_update(self):\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "    def _plot(self,frame_idx,scores,losses,epsilons):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
    "        plt.plot(scores)\n",
    "        plt.subplot(132)\n",
    "        plt.title('loss')\n",
    "        plt.plot(losses)\n",
    "        plt.subplot(133)\n",
    "        plt.title('epsilons')\n",
    "        plt.plot(epsilons)\n",
    "        plt.show()\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic =True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "#主函数\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\",max_episode_steps=500,render_mode = \"rgb_array\")\n",
    "\n",
    "    seed = 777\n",
    "    np.random.seed(seed)\n",
    "    seed_torch(seed)\n",
    "\n",
    "    #设置参数\n",
    "    num_frames = 10000\n",
    "    memory_size = 1000\n",
    "    batch_size = 32\n",
    "    target_update = 100\n",
    "    epsilon_decay = 1/2000\n",
    "\n",
    "    agent = DQN(env=env,memory_size=memory_size,batch_size=batch_size,target_update=target_update,epsilon_decay=epsilon_decay,seed=seed)\n",
    "    agent.train(num_frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
