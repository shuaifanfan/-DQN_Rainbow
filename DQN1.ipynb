{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from typing import Dict,List,Tuple\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,obs_dim:int,size:int,batch_size:int =32):\n",
    "        self.obs_buf = np.zeros([size,obs_dim],dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size,obs_dim],dtype = np.float32)\n",
    "        self.acts_buf = np.zeros([size],dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size],dtype = np.float32)\n",
    "        self.done_buf = np.zeros(size,dtype = np.float32)\n",
    "        self.maxsize,self.batch_size = size,batch_size\n",
    "        self.ptr,self.size = 0,0\n",
    "    def store(\n",
    "            self,\n",
    "            obs:np.ndarray,\n",
    "            act:np.ndarray,\n",
    "            rew:float,\n",
    "            next_obs:np.ndarray,\n",
    "            done:bool\n",
    "    ):\n",
    "        self.obs_buf[self.ptr]=obs\n",
    "        self.next_obs_buf[self.ptr]=next_obs\n",
    "        self.acts_buf[self.ptr]=act\n",
    "        self.rews_buf[self.ptr]=rew\n",
    "        self.done_buf[self.ptr]=done\n",
    "        self.ptr = (self.ptr+1)%self.maxsize\n",
    "        self.size = min(self.size+1,self.maxsize)\n",
    "    def sample_batch(self) -> Dict[str,np.ndarray]:\n",
    "        idxs = np.random.choice(self.size,self.batch_size,replace=False)\n",
    "        return dict(\n",
    "            obs = self.obs_buf[idxs],\n",
    "            next_obs = self.next_obs_buf[idxs],\n",
    "            acts = self.acts_buf[idxs],\n",
    "            rews = self.rews_buf[idxs],\n",
    "            done = self.done_buf[idxs]\n",
    "            )\n",
    "    def __len__(self) ->int:\n",
    "        return self.size\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        super(Network,self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,out_dim)\n",
    "        )\n",
    "    def forward(self,x:torch.Tensor) ->torch.Tensor:\n",
    "            return self.layers(x)\n",
    "\n",
    "class DQN:\n",
    "    def __init__(\n",
    "               self,\n",
    "               env:gym.Env,\n",
    "               memory_size:int,\n",
    "               batch_size:int,\n",
    "               target_update:int,\n",
    "               epsilon_decay:float,\n",
    "               seed:int,\n",
    "               max_epsilon:float =1.0,\n",
    "               min_epsilon:float =0.1,\n",
    "               gamma:float = 0.99\n",
    "     ):\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space.n\n",
    "        self.env = env\n",
    "        self.memory = ReplayBuffer(obs_dim,memory_size,batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.seed = seed\n",
    "        self.epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.gamma = gamma\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.dqn = Network(obs_dim,act_dim).to(self.device)\n",
    "        self.dqn_target = Network(obs_dim,act_dim).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()                                      # target network is not trainable\n",
    "\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(),lr = 1e-3)  #学习率需要调整\n",
    "        self.transition = list()\n",
    "        self.is_test = False\n",
    "    \n",
    "    def select_action(self,state:np.ndarray) ->np.ndarray:\n",
    "         if self.epsilon > np.random.random():\n",
    "             selected_action = self.env.action_space.sample()\n",
    "         else:\n",
    "             selected_action = self.dqn(torch.FloatTensor(state).to(self.device)).argmax() \n",
    "             #argmax()默认方法keepdim=False，因此相当于argmax(self.dqn(torch.FloatTensor(state)),dim=0,keepdim=False),返回值是零维张量\n",
    "             #若 argmax(dim=0,keepdim=True)返回值是一维张量\n",
    "             selected_action = selected_action.detach().cpu().numpy()    #将tensor转化为numpy,并且去掉梯度信息\n",
    "         if not self.is_test:\n",
    "             self.transition = [state,selected_action]          #将state和action存储起来,等待下一步的reward和next_state\n",
    "         return selected_action\n",
    "     \n",
    "    def step(self,action:np.ndarray) -> Tuple[np.ndarray,np.float64,bool]:\n",
    "        next_state,reward,terminated,truncated,_ = self.env.step(action)\n",
    "        done = terminated or truncated                             #done表示是否结束,\n",
    "        if not self.is_test:\n",
    "            self.transition += [reward,next_state,done]            #将reward和next_state,done存储起来\n",
    "            self.memory.store(*self.transition)                    #将transition存储到memory中\n",
    "        return next_state,reward,done\n",
    "    def update_model(self) -> torch.Tensor:\n",
    "        samples = self.memory.sample_batch()\n",
    "        loss = self._compute_dqn_loss(samples)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()  #.item()表示返回tensor中的元素值,不是tensor本身，不是tensor类型，所以不能反向传播\n",
    "    def _compute_dqn_loss(self,samples:Dict[str,np.ndarray]) -> torch.Tensor:\n",
    "        device = self.device\n",
    "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
    "        #下面设个tensor维度重构很重要，因为action,reward,done都是一维的，\n",
    "        #求curr_q_value时，需要按行取值，使用gather()函数，需要保证action的维度是[batch_size,1]\n",
    "        #action从行向量转化为列向量，reshape(-1,1)\n",
    "        action = torch.LongTensor(samples[\"acts\"]).to(device).reshape(-1,1)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"]).to(device).reshape(-1,1)\n",
    "        done = torch.FloatTensor(samples[\"done\"]).to(device).reshape(-1,1)\n",
    "\n",
    "        #计算Q(s_t,a_t),\n",
    "        #网络的输出层维度是action数目\n",
    "        #每次计算loss，都是一个批次的数据计算，在这里假设是cartpole，action数目为2，所以输出层维度为2\n",
    "        #self.dqn(state).shape = [batch_size,2],action.shape = [batch_size,1],按照gather，表示取每一行的action对应的值\n",
    "        #因此，curr_q_value.shape = [batch_size,1]\n",
    "        curr_q_value = self.dqn(state).gather(1,action)  #gather(1,action)表示按行取action对应的值\n",
    "        #计算Q(s_t+1,a_t+1)\n",
    "        next_q_value = self.dqn_target(next_state).max(dim=1,keepdim=True)[0].detach() #dim=1表示按行取最大值,[0]表示取值,[1]表示取索引\n",
    "        \n",
    "        mask = 1-done\n",
    "        target = (reward+self.gamma*next_q_value*mask).to(self.device)\n",
    "        loss = F.smooth_l1_loss(curr_q_value,target)\n",
    "        return loss\n",
    "    def train(self,num_frames,plotting_interval=200):\n",
    "        self.is_test = False\n",
    "        state,_ = self.env.reset(seed=self.seed)\n",
    "        update_cnt = 0\n",
    "        epsilons = []\n",
    "        losses = []\n",
    "        scores = []\n",
    "        score = 0\n",
    "\n",
    "        for frame_idx in range(1,num_frames+1):\n",
    "            action = self.select_action(state)\n",
    "            next_state,reward,done = self.step(action)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                state,_ = self.env.reset(seed=self.seed)\n",
    "                scores.append(score)\n",
    "                score = 0\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                loss = self.update_model()\n",
    "                losses.append(loss)\n",
    "                update_cnt += 1\n",
    "\n",
    "                self.epsilon = max(self.min_epsilon,self.epsilon - (self.max_epsilon-self.min_epsilon)*self.epsilon_decay)\n",
    "                epsilons.append(self.epsilon)\n",
    "\n",
    "                if update_cnt % self.target_update == 0:\n",
    "                    self._target_hard_update()\n",
    "            \n",
    "            if frame_idx % plotting_interval == 0:\n",
    "                self._plot(frame_idx,scores,losses,epsilons)\n",
    "        \n",
    "        self.env.close()\n",
    "    \n",
    "    def _target_hard_update(self):\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "    def _plot(self,frame_idx,scores,losses,epsilons):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
    "        plt.plot(scores)\n",
    "        plt.subplot(132)\n",
    "        plt.title('loss')\n",
    "        plt.plot(losses)\n",
    "        plt.subplot(133)\n",
    "        plt.title('epsilons')\n",
    "        plt.plot(epsilons)\n",
    "        plt.show()\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic =True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "#主函数\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\",max_episode_steps=500,render_mode = \"rgb_array\")\n",
    "\n",
    "    seed = 777\n",
    "    np.random.seed(seed)\n",
    "    seed_torch(seed)\n",
    "\n",
    "    #设置参数\n",
    "    num_frames = 10000\n",
    "    memory_size = 1000\n",
    "    batch_size = 32\n",
    "    target_update = 100\n",
    "    epsilon_decay = 1/2000\n",
    "\n",
    "    agent = DQN(env=env,memory_size=memory_size,batch_size=batch_size,target_update=target_update,epsilon_decay=epsilon_decay,seed=seed)\n",
    "    agent.train(num_frames)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
