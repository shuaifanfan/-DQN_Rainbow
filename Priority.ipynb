{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相对于DQN的改进，在\n",
    "- PrioritizedRepalyBuffer继承父类 (import segment_tree )\n",
    "- DQN agent init()部分\n",
    "- update_model (weigh)\n",
    "- train（）部分要逐步增大beta，损失函数权重慢慢趋于相同\n",
    "  \n",
    "**有关优先采样的论文分析**\n",
    "\n",
    "https://aijishu.com/a/1060000000116781\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/337171931\n",
    "\n",
    "**有关重要性采样的分析**\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/41217212\n",
    "\n",
    "在求蒙特卡洛积分时，采样按照概率分布p(x)采样，则求积分是，需要对每一项采样进行重要性加权，称为重要性权重。\n",
    "\n",
    "同理，在神经网络中，对训练样本按照某个概率分布p(x)输入到网络中，计算损失函数时，需要对每一项损失进行加权，也叫重要性采样。防止bias产生。\n",
    "\n",
    "若仍按照原来的损失函数进行，则网络的输入数据会产生误差/便宜，拟合肯定不准确。所以要对高频率输入的数据降低重要性，对低重要性，低频率的数据的loss增加重要性，防止偏置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'weights' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\lib.fanfan\\study\\强化学习\\Experiment\\rainbow_4\\Priority.ipynb 单元格 2\u001b[0m line \u001b[0;36m<cell line: 317>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=328'>329</a>\u001b[0m epsilon_decay \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2000\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=330'>331</a>\u001b[0m agent \u001b[39m=\u001b[39m DQN(env\u001b[39m=\u001b[39menv,memory_size\u001b[39m=\u001b[39mmemory_size,batch_size\u001b[39m=\u001b[39mbatch_size,target_update\u001b[39m=\u001b[39mtarget_update,epsilon_decay\u001b[39m=\u001b[39mepsilon_decay,seed\u001b[39m=\u001b[39mseed)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=331'>332</a>\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(num_frames)\n",
      "\u001b[1;32me:\\lib.fanfan\\study\\强化学习\\Experiment\\rainbow_4\\Priority.ipynb 单元格 2\u001b[0m line \u001b[0;36mDQN.train\u001b[1;34m(self, num_frames, plotting_interval)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=276'>277</a>\u001b[0m     score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=277'>278</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=278'>279</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_model()\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=279'>280</a>\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=280'>281</a>\u001b[0m     update_cnt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32me:\\lib.fanfan\\study\\强化学习\\Experiment\\rainbow_4\\Priority.ipynb 单元格 2\u001b[0m line \u001b[0;36mDQN.update_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=209'>210</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_model\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=210'>211</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m     \u001b[39m#改变部分\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=212'>213</a>\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39msample_batch(beta\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m     weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(samples[weights]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=214'>215</a>\u001b[0m     indices \u001b[39m=\u001b[39m samples[\u001b[39m\"\u001b[39m\u001b[39mindices\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/lib.fanfan/study/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Experiment/rainbow_4/Priority.ipynb#W1sZmlsZQ%3D%3D?line=215'>216</a>\u001b[0m     elementwise_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dqn_loss(samples)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'weights' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from typing import Dict,List,Tuple\n",
    "from segment_tree import MinSegmentTree, SumSegmentTree\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,obs_dim:int,size:int,batch_size:int =32):\n",
    "        self.obs_buf = np.zeros([size,obs_dim],dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size,obs_dim],dtype = np.float32)\n",
    "        self.acts_buf = np.zeros([size],dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size],dtype = np.float32)\n",
    "        self.done_buf = np.zeros(size,dtype = np.float32)\n",
    "        self.maxsize,self.batch_size = size,batch_size\n",
    "        self.ptr,self.size = 0,0\n",
    "    def store(\n",
    "            self,\n",
    "            obs:np.ndarray,\n",
    "            act:np.ndarray,\n",
    "            rew:float,\n",
    "            next_obs:np.ndarray,\n",
    "            done:bool\n",
    "    ):\n",
    "        self.obs_buf[self.ptr]=obs\n",
    "        self.next_obs_buf[self.ptr]=next_obs\n",
    "        self.acts_buf[self.ptr]=act\n",
    "        self.rews_buf[self.ptr]=rew\n",
    "        self.done_buf[self.ptr]=done\n",
    "        self.ptr = (self.ptr+1)%self.maxsize\n",
    "        self.size = min(self.size+1,self.maxsize)\n",
    "    def sample_batch(self) -> Dict[str,np.ndarray]:\n",
    "        idxs = np.random.choice(self.size,self.batch_size,replace=False)\n",
    "        return dict(\n",
    "            obs = self.obs_buf[idxs],\n",
    "            next_obs = self.next_obs_buf[idxs],\n",
    "            acts = self.acts_buf[idxs],\n",
    "            rews = self.rews_buf[idxs],\n",
    "            done = self.done_buf[idxs]\n",
    "            )\n",
    "    def __len__(self) ->int:\n",
    "        return self.size\n",
    "\n",
    "#优先级继承\n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self,obs_dim,size,batch_size = 32,alpha = 0.6):\n",
    "        assert alpha >= 0\n",
    "        super(PrioritizedReplayBuffer,self).__init__(obs_dim,size,batch_size) #调用父类的构造函数\n",
    "        self.alpha = alpha\n",
    "        self.max_priority = 1.0\n",
    "        self.tree_ptr = 0\n",
    "\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < self.maxsize:\n",
    "            tree_capacity *=2\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "    \n",
    "    def store(\n",
    "            self,\n",
    "            obs:np.ndarray,\n",
    "            act:int,\n",
    "            rew:float,\n",
    "            next_obs:np.ndarray,\n",
    "            done:bool\n",
    "    ):\n",
    "        super().store(obs,act,rew,next_obs,done)\n",
    "        self.sum_tree[self.tree_ptr] = self.max_priority**self.alpha\n",
    "        self.min_tree[self.tree_ptr] = self.max_priority**self.alpha\n",
    "        self.tree_ptr = (self.tree_ptr+1)%self.maxsize\n",
    "\n",
    "    def sample_batch(self,beta = 0.4) -> Dict[str,np.ndarray]:\n",
    "        assert beta >0\n",
    "        assert len(self) >= self.batch_size\n",
    "\n",
    "        indices = self._sample_proportional()\n",
    "\n",
    "        obs = self.obs_buf[indices]\n",
    "        next_obs = self.next_obs_buf[indices]\n",
    "        acts = self.acts_buf[indices]\n",
    "        rews = self.rews_buf[indices]\n",
    "        done = self.done_buf[indices]\n",
    "        weights = np.array([self._calculate_weight(i,beta) for i in indices])\n",
    "\n",
    "        return dict(obs = obs,\n",
    "                    next_obs = next_obs,\n",
    "                    acts = acts,\n",
    "                    rews = rews,\n",
    "                    done = done,\n",
    "                    weights = weights,\n",
    "                    indices = indices\n",
    "                    )\n",
    "    def update_priorities(self,indices:List[int],priorities:np.ndarray):\n",
    "        assert len(indices) == len(priorities)\n",
    "        for idx,priority in zip(indices,priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self)\n",
    "            self.sum_tree[idx] = priority**self.alpha\n",
    "            self.min_tree[idx] = priority**self.alpha\n",
    "            self.max_priority = max(self.max_priority,priority)\n",
    "    \n",
    "    def _sample_proportional(self) -> List[int]:\n",
    "        indices = []\n",
    "        p_total = self.sum_tree.sum(0,len(self)-1)\n",
    "        segment = p_total/self.batch_size\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            a = segment*i\n",
    "            b = segment*(i+1)\n",
    "            upperbound = np.random.uniform(a,b)\n",
    "            idx = self.sum_tree.retrieve(upperbound)\n",
    "            indices.append(idx)\n",
    "        return indices\n",
    "    \n",
    "    def _calculate_weight(self,idx,beta:float):\n",
    "        \"\"\"\n",
    "        由于采用加重了一部分数据采样的概率，因此在损失函数中，需要降低这些函数的权重，否则会导致bias\n",
    "        计算权重,其实这个权重的计算公式和论文中的不一样，论文中的是w_i = (N*P(i))**(-beta),这里的是w_i = (p_min*N)**(-beta)\n",
    "        论文中的是平均采样的概率，这里的是最小采样的概率，影响的是bias的程度，影响不大\n",
    "        \"\"\"\n",
    "        pin_min = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_weight = (pin_min*len(self)) ** (-beta)\n",
    "\n",
    "        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
    "        weight = (p_sample*len(self))** (-beta)\n",
    "        weight = weight / max_weight\n",
    "        return weight\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        super(Network,self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,out_dim)\n",
    "        )\n",
    "    def forward(self,x:torch.Tensor) ->torch.Tensor:\n",
    "            return self.layers(x)\n",
    "\n",
    "class DQN:\n",
    "    def __init__(\n",
    "               self,\n",
    "               env:gym.Env,\n",
    "               memory_size:int,\n",
    "               batch_size:int,\n",
    "               target_update:int,\n",
    "               epsilon_decay:float,\n",
    "               seed:int,\n",
    "               max_epsilon:float =1.0,\n",
    "               min_epsilon:float =0.1,\n",
    "               gamma:float = 0.99,\n",
    "               \n",
    "               #Priority_change_begin\n",
    "               alpha:float = 0.2,\n",
    "               beta:float = 0.6,\n",
    "               prior_eps:float = 1e-6\n",
    "     ):\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space.n\n",
    "        self.env = env\n",
    "        #改变部分\n",
    "        self.memory = PrioritizedReplayBuffer(obs_dim,memory_size,batch_size,alpha)\n",
    "        self.beta = beta\n",
    "        self.prior_eps = prior_eps\n",
    "        #改变部分结束\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.seed = seed\n",
    "        self.epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.gamma = gamma\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.dqn = Network(obs_dim,act_dim).to(self.device)\n",
    "        self.dqn_target = Network(obs_dim,act_dim).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()                                      # target network is not trainable\n",
    "\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(),lr = 1e-3)  #学习率需要调整\n",
    "        self.transition = list()\n",
    "        self.is_test = False\n",
    "    \n",
    "    def select_action(self,state:np.ndarray) ->np.ndarray:\n",
    "         if self.epsilon > np.random.random():\n",
    "             selected_action = self.env.action_space.sample()\n",
    "         else:\n",
    "             selected_action = self.dqn(torch.FloatTensor(state).to(self.device)).argmax() \n",
    "             #argmax()默认方法keepdim=False，因此相当于argmax(self.dqn(torch.FloatTensor(state)),dim=0,keepdim=False),返回值是零维张量\n",
    "             #若 argmax(dim=0,keepdim=True)返回值是一维张量\n",
    "             selected_action = selected_action.detach().cpu().numpy()    #将tensor转化为numpy,并且去掉梯度信息\n",
    "         if not self.is_test:\n",
    "             self.transition = [state,selected_action]          #将state和action存储起来,等待下一步的reward和next_state\n",
    "         return selected_action\n",
    "     \n",
    "    def step(self,action:np.ndarray) -> Tuple[np.ndarray,np.float64,bool]:\n",
    "        next_state,reward,terminated,truncated,_ = self.env.step(action)\n",
    "        done = terminated or truncated                             #done表示是否结束,\n",
    "        if not self.is_test:\n",
    "            self.transition += [reward,next_state,done]            #将reward和next_state,done存储起来\n",
    "            self.memory.store(*self.transition)                    #将transition存储到memory中\n",
    "        return next_state,reward,done\n",
    "    def update_model(self) -> torch.Tensor:\n",
    "\n",
    "        #改变部分\n",
    "        samples = self.memory.sample_batch(beta=self.beta)\n",
    "        weights = torch.FloatTensor(samples[\"weights\"].reshape(-1,1)).to(self.device)\n",
    "        indices = samples[\"indices\"]\n",
    "        elementwise_loss = self._compute_dqn_loss(samples)\n",
    "        loss = torch.mean(elementwise_loss*weights) \n",
    "        #其实求均值/求和，然后再方向传播 和 对一个loss列向量（张量）直接反向传播是一样的，因为计算图。。。\n",
    "        # loss = F.smooth_l1_loss(curr_q_value,target,reduction=\"none\"),reduction表示对loss求和/求均值，none表示返回列向量\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_for_prior = elementwise_loss.detach().cpu().numpy()  #单个transition的loss就是δ，δ = |Q(s_t,a_t)-[r_t+γ*max_aQ(s_t+1,a)]|\n",
    "        new_priorities = loss_for_prior + self.prior_eps        #软化因子，防止除零情况\n",
    "        self.memory.update_priorities(indices,new_priorities)\n",
    "        return loss.item()  #.item()表示返回tensor中的元素值,不是tensor本身，不是tensor类型，所以不能反向传播\n",
    "        #改变部分结束\n",
    "\n",
    "    def _compute_dqn_loss(self,samples:Dict[str,np.ndarray]) -> torch.Tensor:\n",
    "        device = self.device\n",
    "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
    "        #下面设个tensor维度重构很重要，因为action,reward,done都是一维的，\n",
    "        #求curr_q_value时，需要按行取值，使用gather()函数，需要保证action的维度是[batch_size,1]\n",
    "        #action从行向量转化为列向量，reshape(-1,1)\n",
    "        action = torch.LongTensor(samples[\"acts\"]).to(device).reshape(-1,1)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"]).to(device).reshape(-1,1)\n",
    "        done = torch.FloatTensor(samples[\"done\"]).to(device).reshape(-1,1)\n",
    "\n",
    "        #计算Q(s_t,a_t),\n",
    "        #网络的输出层维度是action数目\n",
    "        #每次计算loss，都是一个批次的数据计算，在这里假设是cartpole，action数目为2，所以输出层维度为2\n",
    "        #self.dqn(state).shape = [batch_size,2],action.shape = [batch_size,1],按照gather，表示取每一行的action对应的值\n",
    "        #因此，curr_q_value.shape = [batch_size,1]\n",
    "        curr_q_value = self.dqn(state).gather(1,action)  #gather(1,action)表示按行取action对应的值\n",
    "        #计算Q(s_t+1,a_t+1)\n",
    "        next_q_value = self.dqn_target(next_state).max(dim=1,keepdim=True)[0].detach() #dim=1表示按行取最大值,[0]表示取值,[1]表示取索引\n",
    "        \n",
    "        mask = 1-done\n",
    "        target = (reward+self.gamma*next_q_value*mask).to(self.device)\n",
    "\n",
    "        loss = F.smooth_l1_loss(curr_q_value,target,reduction=\"none\")\n",
    "        return loss\n",
    "    def train(self,num_frames,plotting_interval=200):\n",
    "        self.is_test = False\n",
    "        state,_ = self.env.reset(seed=self.seed)\n",
    "        update_cnt = 0\n",
    "        epsilons = []\n",
    "        losses = []\n",
    "        scores = []\n",
    "        score = 0\n",
    "\n",
    "        for frame_idx in range(1,num_frames+1):\n",
    "            action = self.select_action(state)\n",
    "            next_state,reward,done = self.step(action)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            #逐步增大beta\n",
    "            fraction = min(frame_idx/num_frames,1.0)\n",
    "            self.beta = self.beta + fraction*(1.0-self.beta)\n",
    "\n",
    "            if done:\n",
    "                state,_ = self.env.reset(seed=self.seed)\n",
    "                scores.append(score)\n",
    "                score = 0\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                loss = self.update_model()\n",
    "                losses.append(loss)\n",
    "                update_cnt += 1\n",
    "\n",
    "                self.epsilon = max(self.min_epsilon,self.epsilon - (self.max_epsilon-self.min_epsilon)*self.epsilon_decay)\n",
    "                epsilons.append(self.epsilon)\n",
    "\n",
    "                if update_cnt % self.target_update == 0:\n",
    "                    self._target_hard_update()\n",
    "            \n",
    "            if frame_idx % plotting_interval == 0:\n",
    "                self._plot(frame_idx,scores,losses,epsilons)\n",
    "        \n",
    "        self.env.close()\n",
    "    \n",
    "    def _target_hard_update(self):\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "    def _plot(self,frame_idx,scores,losses,epsilons):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
    "        plt.plot(scores)\n",
    "        plt.subplot(132)\n",
    "        plt.title('loss')\n",
    "        plt.plot(losses)\n",
    "        plt.subplot(133)\n",
    "        plt.title('epsilons')\n",
    "        plt.plot(epsilons)\n",
    "        plt.show()\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic =True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "#主函数\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\",max_episode_steps=500,render_mode = \"rgb_array\")\n",
    "\n",
    "    seed = 777\n",
    "    np.random.seed(seed)\n",
    "    seed_torch(seed)\n",
    "\n",
    "    #设置参数\n",
    "    num_frames = 10000\n",
    "    memory_size = 1000\n",
    "    batch_size = 32\n",
    "    target_update = 100\n",
    "    epsilon_decay = 1/2000\n",
    "\n",
    "    agent = DQN(env=env,memory_size=memory_size,batch_size=batch_size,target_update=target_update,epsilon_decay=epsilon_decay,seed=seed)\n",
    "    agent.train(num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
